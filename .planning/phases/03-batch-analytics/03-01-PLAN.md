---
phase: 03-batch-analytics
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/app/core/config.py
  - backend/app/services/batch/result_aggregator.py
  - backend/app/services/batch/tasks.py
  - backend/app/services/batch/analytics_tasks.py
  - backend/app/services/analytics/__init__.py
  - backend/app/services/analytics/storage.py
  - backend/app/schemas/analytics.py
  - backend/app/api/routes/batch.py
  - backend/tests/test_infra_analytics.py
autonomous: true
requirements:
  - INFRA-01

must_haves:
  truths:
    - "Batch results are stored with a 24h TTL (86400 seconds) instead of 1h"
    - "result_storage.get_all_results(job_id) returns all raw results without pagination for analytics consumption"
    - "GET /api/v1/batch/{job_id}/analytics returns analytics status and cached results"
    - "POST /api/v1/batch/{job_id}/analytics/{analysis_type} triggers an expensive analytics Celery task"
    - "aggregate_batch_results dispatches run_cheap_analytics.delay(job_id) after storing results"
    - "Analytics results are cached under batch:analytics:{type}:{job_id} keys with 24h TTL"
  artifacts:
    - path: "backend/app/core/config.py"
      provides: "BATCH_RESULT_TTL setting"
      contains: "BATCH_RESULT_TTL"
    - path: "backend/app/services/batch/result_aggregator.py"
      provides: "get_all_results method and TTL from config"
      contains: "get_all_results"
    - path: "backend/app/services/batch/analytics_tasks.py"
      provides: "Celery tasks for cheap and expensive analytics"
      exports: ["run_cheap_analytics", "run_expensive_analytics"]
    - path: "backend/app/services/analytics/storage.py"
      provides: "Analytics result storage/retrieval in Redis"
      exports: ["analytics_storage"]
    - path: "backend/app/schemas/analytics.py"
      provides: "Pydantic schemas for all analytics responses"
      contains: "BatchAnalyticsResponse"
    - path: "backend/app/api/routes/batch.py"
      provides: "Analytics GET and POST endpoints"
      contains: "get_batch_analytics"
  key_links:
    - from: "backend/app/services/batch/tasks.py"
      to: "backend/app/services/batch/analytics_tasks.py"
      via: "run_cheap_analytics.delay(job_id) call in aggregate_batch_results"
      pattern: "run_cheap_analytics\\.delay"
    - from: "backend/app/api/routes/batch.py"
      to: "backend/app/services/analytics/storage.py"
      via: "analytics_storage.get_status() and get_result() in route handler"
      pattern: "analytics_storage"
    - from: "backend/app/services/batch/result_aggregator.py"
      to: "backend/app/core/config.py"
      via: "settings.BATCH_RESULT_TTL for TTL value"
      pattern: "settings\\.BATCH_RESULT_TTL"
---

<objective>
Deliver INFRA-01 (batch result pagination TTL policy) and the analytics infrastructure skeleton that all subsequent Phase 3 plans build upon.

Purpose: INFRA-01 is the prerequisite for all analytics — without 24h TTL, analytics tasks would find expired batch results. The analytics skeleton (storage, schemas, routes, task dispatcher) establishes the shared foundation so plans 03-02 through 03-06 only need to implement their individual service modules.

Output: Working TTL policy, analytics endpoints, Celery task dispatcher, analytics storage utility — all wired and tested.
</objective>

<execution_context>
@/Users/kohulanrajan/.claude/get-shit-done/workflows/execute-plan.md
@/Users/kohulanrajan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-batch-analytics/03-RESEARCH.md

@backend/app/core/config.py
@backend/app/services/batch/result_aggregator.py
@backend/app/services/batch/tasks.py
@backend/app/api/routes/batch.py
@backend/app/schemas/batch.py
@backend/app/celery_app.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: INFRA-01 TTL Policy and get_all_results</name>
  <files>
    backend/app/core/config.py
    backend/app/services/batch/result_aggregator.py
  </files>
  <action>
1. In `backend/app/core/config.py`, add a `BATCH_RESULT_TTL: int = 86400` setting (24 hours in seconds) in the Caching section, with a comment: "# Batch result TTL — 24h to support analytics (INFRA-01)".

2. In `backend/app/services/batch/result_aggregator.py`:
   a. Change `RESULT_EXPIRY = 3600` to `RESULT_EXPIRY = settings.BATCH_RESULT_TTL` (import settings if not already imported — it IS already imported).
   b. Add a new method `get_all_results(self, job_id: str) -> list[dict]` on the `ResultStorage` class:
      ```python
      def get_all_results(self, job_id: str) -> list[dict]:
          """Return all raw results (no pagination) for analytics computation.

          Used by analytics tasks that need the full dataset.
          Returns empty list if results have expired or don't exist.
          """
          r = self._get_redis()
          data = r.get(f"batch:results:{job_id}")
          if not data:
              return []
          return json.loads(data)
      ```
   c. Keep the `VIEW_CACHE_EXPIRY = 300` as-is (view caches are short-lived by design).
  </action>
  <verify>
    <automated>cd /Volumes/Data_Drive/Project/2026/chemstructval/backend && /Users/kohulanrajan/anaconda3/envs/cheminformatics/bin/python -c "from app.core.config import settings; assert settings.BATCH_RESULT_TTL == 86400; print('TTL OK')" && /Users/kohulanrajan/anaconda3/envs/cheminformatics/bin/python -c "from app.services.batch.result_aggregator import ResultStorage; assert hasattr(ResultStorage, 'get_all_results'); print('get_all_results OK')"</automated>
  </verify>
  <done>BATCH_RESULT_TTL=86400 in config, RESULT_EXPIRY reads from config, get_all_results method exists on ResultStorage</done>
</task>

<task type="auto">
  <name>Task 2: Analytics Infrastructure Skeleton (Storage, Schemas, Routes, Tasks)</name>
  <files>
    backend/app/services/analytics/__init__.py
    backend/app/services/analytics/storage.py
    backend/app/services/batch/analytics_tasks.py
    backend/app/schemas/analytics.py
    backend/app/api/routes/batch.py
    backend/app/services/batch/tasks.py
    backend/tests/test_infra_analytics.py
  </files>
  <action>
1. Create `backend/app/services/analytics/__init__.py` — empty or with docstring "Analytics services for batch post-processing."

2. Create `backend/app/services/analytics/storage.py` — AnalyticsStorage class:
   - Constructor takes `redis_url: str = None`, lazy Redis connection (same pattern as ResultStorage).
   - `ANALYTICS_TTL = 86400` (24h).
   - Key format: `batch:analytics:{analysis_type}:{job_id}` for results, `batch:analytics:status:{job_id}` for status.
   - Methods:
     - `store_result(job_id, analysis_type, data: dict, ttl: int = ANALYTICS_TTL)` — JSON-serialize data, store with TTL.
     - `get_result(job_id, analysis_type) -> dict | None` — deserialize and return, or None.
     - `get_status(job_id) -> dict | None` — read status dict from `batch:analytics:status:{job_id}`.
     - `update_status(job_id, analysis_type, status: str, error: str | None = None)` — read current status dict, update the given analysis_type's sub-key, write back with TTL. Initial status dict structure: `{"deduplication": {"status": "pending"}, "scaffold": {...}, ...}`. Use `setdefault` for new analysis types.
     - `init_status(job_id, auto_analyses: list[str])` — create initial status dict with "computing" for auto analyses and "pending" for all others.
   - Singleton: `analytics_storage = AnalyticsStorage()`

3. Create `backend/app/schemas/analytics.py` — Pydantic v2 schemas:
   - `AnalysisStatus(BaseModel)`: `status: Literal["pending", "computing", "complete", "failed", "skipped"]`, `computed_at: Optional[float] = None`, `error: Optional[str] = None`
   - `DeduplicationGroup(BaseModel)`: `level: str`, `representative_index: int`, `duplicate_indices: list[int]`, `group_key: str`, `count: int`
   - `DeduplicationResult(BaseModel)`: `exact: list[DeduplicationGroup]`, `tautomeric: list[DeduplicationGroup]`, `stereo_insensitive: list[DeduplicationGroup]`, `salt_form: list[DeduplicationGroup]`, `total_unique: dict[str, int]` (counts per level)
   - `ScaffoldGroup(BaseModel)`: `scaffold_smiles: str`, `generic_scaffold_smiles: str`, `molecule_indices: list[int]`, `count: int`
   - `ScaffoldResult(BaseModel)`: `scaffolds: list[ScaffoldGroup]`, `unique_scaffold_count: int`, `shannon_entropy: float`, `frequency_distribution: dict[str, int]`
   - `RGroupResult(BaseModel)`: `core_smarts: str`, `decomposition: list[dict]`, `unmatched_count: int`
   - `ChemSpaceCoordinates(BaseModel)`: `method: Literal["pca", "tsne"]`, `coordinates: list[list[float]]`, `molecule_indices: list[int]`, `variance_explained: Optional[list[float]] = None` (PCA only)
   - `SimilarityHit(BaseModel)`: `index: int`, `similarity: float`, `smiles: str`
   - `SimilarityResult(BaseModel)`: `query_index: Optional[int]`, `query_smiles: Optional[str]`, `neighbors: list[SimilarityHit]`
   - `NearestNeighborResult(BaseModel)`: `molecule_index: int`, `nearest_index: int`, `similarity: float`, `isolation_score: float`
   - `SimilarityMatrixResult(BaseModel)`: `size: int`, `representation: Literal["dense", "sparse"]`, `data: list` (dense: full upper-triangle, sparse: list of {i, j, similarity} dicts for sim > threshold)
   - `MMPPair(BaseModel)`: `mol_a_index: int`, `mol_b_index: int`, `core_smiles: str`, `rgroup_a: str`, `rgroup_b: str`, `tanimoto: float`
   - `ActivityCliff(BaseModel)`: `mol_a_index: int`, `mol_b_index: int`, `sali: float`, `tanimoto: float`, `activity_diff: float`
   - `MMPResult(BaseModel)`: `pairs: list[MMPPair]`, `activity_cliffs: Optional[list[ActivityCliff]]`, `lle_values: Optional[list[dict]]`
   - `PropertyStats(BaseModel)`: `property_name: str`, `mean: float`, `median: float`, `std: float`, `q1: float`, `q3: float`, `iqr: float`, `min: float`, `max: float`, `count: int`
   - `OutlierInfo(BaseModel)`: `molecule_index: int`, `property_name: str`, `value: float`, `lower_fence: float`, `upper_fence: float`
   - `PropertyCorrelation(BaseModel)`: `property_a: str`, `property_b: str`, `pearson_r: float`
   - `QualityScore(BaseModel)`: `score: float`, `validity_pct: float`, `diversity_pct: float`, `druglikeness_pct: float`
   - `StatisticsResult(BaseModel)`: `property_stats: list[PropertyStats]`, `correlations: list[PropertyCorrelation]`, `outliers: list[OutlierInfo]`, `quality_score: QualityScore`
   - `BatchAnalyticsResponse(BaseModel)`: `job_id: str`, `status: dict[str, AnalysisStatus]`, `deduplication: Optional[DeduplicationResult] = None`, `scaffold: Optional[ScaffoldResult] = None`, `chemical_space: Optional[ChemSpaceCoordinates] = None`, `similarity_matrix: Optional[SimilarityMatrixResult] = None`, `mmp: Optional[MMPResult] = None`, `statistics: Optional[StatisticsResult] = None`
   - `AnalyticsTriggerResponse(BaseModel)`: `job_id: str`, `analysis_type: str`, `status: str`

4. Create `backend/app/services/batch/analytics_tasks.py`:
   - Import celery_app from `app.celery_app`.
   - Import `analytics_storage` from `app.services.analytics.storage`.
   - Import `result_storage` from `app.services.batch.result_aggregator`.
   - Task `run_cheap_analytics(self, job_id: str)`:
     - `@celery_app.task(bind=True, queue="default", ignore_result=True)`
     - Get all results via `result_storage.get_all_results(job_id)`.
     - Defensive check: if empty list, log warning and set status to failed with "Batch results expired — resubmit batch". Return early.
     - Init status via `analytics_storage.init_status(job_id, auto_analyses=["deduplication", "statistics"])`.
     - Try import `from app.services.analytics.deduplication import compute_all_dedup_levels`. If ImportError, log warning, skip.
     - If dedup module available: compute deduplication, store result, update status to "complete".
     - Try import `from app.services.analytics.statistics import compute_all_statistics`. If ImportError, log warning, skip.
     - If stats module available: compute statistics, store result, update status to "complete".
     - Use try/except around each analytics computation. On error, update status to "failed" with error message.
   - Task `run_expensive_analytics(self, job_id: str, analysis_type: str, params: dict | None = None)`:
     - `@celery_app.task(bind=True, queue="default", ignore_result=True)`
     - Get all results. Defensive check for empty.
     - Update status to "computing" for the analysis_type.
     - Match analysis_type:
       - "scaffold": import and call scaffold analysis service.
       - "chemical_space": import and call chemical space service. Pass params for method ("pca"/"tsne").
       - "mmp": import and call MMP service. Pass params for activity_column.
       - "similarity_search": import and call similarity search. Pass params for query_smiles, query_index, top_k.
       - "rgroup": import and call R-group decomposition. Pass params for core_smarts.
     - On success: store result, update status to "complete".
     - On error: update status to "failed" with error message.
     - Unknown analysis_type: update status to "failed" with "Unknown analysis type".

5. Extend `backend/app/api/routes/batch.py`:
   - Add imports for analytics schemas (`BatchAnalyticsResponse`, `AnalyticsTriggerResponse`, `AnalysisStatus`).
   - Add import for `analytics_storage`.
   - Add import for `run_expensive_analytics` from analytics_tasks.
   - Add `GET /batch/{job_id}/analytics` endpoint:
     - Rate limit: `30/minute`.
     - Read status from `analytics_storage.get_status(job_id)`.
     - If None, return 404 "Analytics not available for this job".
     - For each analysis type with status "complete", read result from `analytics_storage.get_result(job_id, type)`.
     - Return `BatchAnalyticsResponse` with status dict and available results.
   - Add `POST /batch/{job_id}/analytics/{analysis_type}` endpoint:
     - Rate limit: `10/minute`.
     - Validate analysis_type in allowed set: `{"scaffold", "chemical_space", "mmp", "similarity_search", "rgroup"}`.
     - Read optional JSON body for params (query_smiles, method, activity_column, core_smarts, top_k).
     - Dispatch `run_expensive_analytics.delay(job_id, analysis_type, params)`.
     - Return `AnalyticsTriggerResponse(job_id=job_id, analysis_type=analysis_type, status="queued")`.

6. Modify `backend/app/services/batch/tasks.py`:
   - In `aggregate_batch_results`, after `progress_tracker.mark_complete(job_id)`, add:
     ```python
     # Trigger cheap analytics computation
     try:
         from app.services.batch.analytics_tasks import run_cheap_analytics
         run_cheap_analytics.delay(job_id)
     except Exception:
         import logging
         logging.getLogger(__name__).warning("Failed to dispatch cheap analytics for %s", job_id)
     ```
   - Add the same block to `aggregate_batch_results_priority`.

7. Create `backend/tests/test_infra_analytics.py`:
   - Test `settings.BATCH_RESULT_TTL == 86400`.
   - Test `ResultStorage.get_all_results` returns empty list for nonexistent job (mock Redis).
   - Test `AnalyticsStorage.store_result` and `get_result` round-trip (mock Redis with fakeredis).
   - Test `AnalyticsStorage.init_status` and `update_status` (mock Redis).
   - Test analytics schemas instantiation (DeduplicationGroup, BatchAnalyticsResponse, etc.).
   - Test GET /batch/{job_id}/analytics returns 404 when no analytics exist (use TestClient with mock storage).
  </action>
  <verify>
    <automated>cd /Volumes/Data_Drive/Project/2026/chemstructval/backend && /Users/kohulanrajan/anaconda3/envs/cheminformatics/bin/python -m pytest tests/test_infra_analytics.py -x -v</automated>
  </verify>
  <done>Analytics infrastructure is operational: storage utility stores/retrieves analytics in Redis with 24h TTL, schemas validate all analytics response types, GET/POST analytics endpoints exist in batch routes, run_cheap_analytics dispatched from aggregate_batch_results, run_expensive_analytics dispatches per-type analytics tasks</done>
</task>

</tasks>

<verification>
1. `settings.BATCH_RESULT_TTL` returns 86400
2. `ResultStorage.RESULT_EXPIRY` reads from config (not hardcoded 3600)
3. `get_all_results()` method exists and returns list[dict]
4. `analytics_storage` singleton can store/retrieve JSON data
5. `BatchAnalyticsResponse` schema validates with all optional fields
6. GET /batch/{job_id}/analytics returns 404 for unknown jobs
7. POST /batch/{job_id}/analytics/scaffold returns 200 with queued status
8. Both `aggregate_batch_results` and `aggregate_batch_results_priority` dispatch cheap analytics
</verification>

<success_criteria>
- INFRA-01 fully implemented: 24h TTL configurable via BATCH_RESULT_TTL
- Analytics skeleton operational: storage, schemas, routes, task dispatcher all wired
- Subsequent plans (03-02 through 03-06) only need to create service modules under app/services/analytics/
- All tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/03-batch-analytics/03-01-SUMMARY.md`
</output>
